---
title: "p8105_hw6_cm3341"
author: "Carolina Montes Garcia"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true 
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(p8105.datasets)
library(modelr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## Problem 1

Import data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

I broke down the steps of this problem, probably more than what the professor wants, but I had to in order to understand what I was doing.

First, I set up bootstrap samples, fit the model, extract r squared and computer log (B0*B1). B0 is the y intercept and B1 is the slope.

```{r}

set.seed(41)

bootstrap_samples = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) 

bootstrap_results = 
  bootstrap_samples %>% 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    r_squared = map_dbl(models, \(mod) broom::glance(mod)$r.squared),
    log_beta0_beta1 = map_dbl(models, \(mod) {
      coef_estimates = broom::tidy(mod) %>% 
        select(term, estimate) %>% 
        pivot_wider(names_from = term, values_from = estimate)
      log(coef_estimates$`(Intercept)` * coef_estimates$tmin)
    })
  )

head(bootstrap_results)
```

calculate confidence intervals for each statistic and add them to the dataframe
```{r}

bootstrap_results = 
  bootstrap_results %>%
  mutate(
    r_squared_ci_lower = quantile(r_squared, 0.025),
    r_squared_ci_upper = quantile(r_squared, 0.975),
    log_beta0_beta1_ci_lower = quantile(log_beta0_beta1, 0.025),
    log_beta0_beta1_ci_upper = quantile(log_beta0_beta1, 0.975)
  )

head(bootstrap_results)
```

Distribution plots

```{r}
bootstrap_results %>%
  ggplot(aes(x = r_squared)) +
  geom_histogram(bins = 30)


bootstrap_results %>%
  ggplot(aes(x = log_beta0_beta1)) +
  geom_histogram(bins = 30)
```

The r.squared and log(B0*B1) statistics in the bootstrap samples seem to be relatively normally distributed. 



## Problem 2

Import csv data from the Washington Post GitHub Repository. This dataset includes data on homicides in 50 large US cities. Tidy the data using the code from the class example that used the same dataset. 
```{r}

cities_df = 
  read_csv("data/homicide-data.csv") %>%  
  mutate(
    city_state = paste(city, state, sep = ", "),
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age)) %>%  
  filter(
    !(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),  # Exclude specified cities
    victim_race %in% c("White", "Black")
  ) %>% 
  select(city_state, resolved, victim_age, victim_race, victim_sex) 
```

Fit logistic regression for `city_state` Baltimore, MD.
```{r}
fit_logistic = 
  cities_df %>%  
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```

Run `broom::tidy`
```{r}
fit_logistic %>%  
  broom::tidy(conf.int = TRUE) %>%  
  filter(term == "victim_sexMale") %>%
  mutate(OR = exp(estimate)) %>% 
  select(term, log_OR = estimate, OR, p.value, conf.low = conf.low, 
    conf.high = conf.high) %>%  
  knitr::kable(digits = 3)

```
 
Interpretation: The odds of solving a homicide for male victims are 42.6% of the odds for female victims, keeping other variables constant.


Glm and nesting for all other cities
```{r}
nest_lm_cities = 
  cities_df %>% 
  nest(data = -city_state) %>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, ~broom::tidy(.x, conf.int = TRUE))
  ) %>% 
  select(city_state, results) %>% 
  unnest(cols = results) %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(OR = exp(estimate))
```


Plot showing the estimated ORs and CIs for each city for solving homicides comparing male victims to female victims. 

```{r}

nest_lm_cities %>% 
  arrange(OR) %>%
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() + 
  facet_wrap(~term) + 
  geom_errorbar(aes(ymin = exp(conf.low), ymax = exp(conf.high)), width = 0.2)+
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
  
  
```

Plot interpretation: In this plot, we see that the majority of the points (representing adjusted ORs) fall below 1, a handful of points fall right about 1, and just 3-4 points fall above 1. Points that fall below 1 indicate that male victims in those cities have lower odds to have their homicides resolved compared to female victims, after accounting for victim age and race. Points that fall right around 1 indicate no real difference between the likelihood of male or female victims having their homicide cases resolved, after accounting for victim age and race. Points that fall above 1 indicate that male victims have higher odds of having their homicides resolved compared to female victims, after accounting for victim age and race.

It is worth noting, however, that many of the confidence intervals cross 1, indicating that the adjusted ORs for those cities were not found to be statistically significant. 

## Problem 3

Import and clean data
```{r}
bwt_df = 
  read_csv("data/birthweight.csv",  
  na = c("NA", "."))%>%
  drop_na() %>% 
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("Male", "Female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), 
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    malform = factor(malform, levels = c(0, 1), labels = c("Absent", "Present")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8), 
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
  )
```

Based on risk factors that can contribute to low birth weight listed on the cleveland clinic website (https://my.clevelandclinic.org/health/diseases/24980-low-birth-weight), I chose to model gestational age in weeks, mother's weight at delivery, and mother's age at delivery as predictors of birth weight.

Fit my proposed linear model.
```{r}
my_mod = lm(bwt ~ gaweeks + delwt + momage, data = bwt_df)
summary(my_mod)
```


```{r}
bwt_df = 
  bwt_df %>%
  modelr::add_predictions(my_mod) %>%
  modelr::add_residuals(my_mod)


bwt_df %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5)

bwt_df %>%
  ggplot(aes(x = resid)) +
  geom_histogram(bins = 30)
```
The residuals seem to follow a normal distribution, therefore, I can feel confident in my use of a linear regression model. 


Next, I will create the two other models requested:

* One using length at birth and gestational age as predictors (main effects only)

*One using head circumference, length, sex, and all interactions (including the three-way interaction) between these


Model for length at birth and gestational age as predictors as predictors of birth weight. Check for normality.
```{r}
length_ga_mod = lm(bwt ~ blength + gaweeks, data = bwt_df)
summary(length_ga_mod)

bwt_df %>%
  modelr::add_predictions(length_ga_mod) %>%
  modelr::add_residuals(length_ga_mod) %>%
  ggplot(aes(x = pred, y = resid))+
  geom_point(alpha = 0.5)

bwt_df %>%
  modelr::add_predictions(length_ga_mod) %>%
  modelr::add_residuals(length_ga_mod) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(bins = 30)
```
There seem to be some outliers on the upper end of the distribution. The histogram is quite narrow as well. 


Model for head circumference, baby length, baby sex, and all interactions as predictors of birth weight

```{r}
interactions_model = lm(bwt ~ bhead * blength * babysex, data = bwt_df)
summary(interactions_model)

bwt_df %>%
  modelr::add_predictions(interactions_model) %>%
  modelr::add_residuals(interactions_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.5)

bwt_df %>%
  modelr::add_predictions(interactions_model) %>%
  modelr::add_residuals(interactions_model) %>%
  ggplot(aes(x = resid)) +
  geom_histogram(bins = 30)


```
Likewise, this distribution also shows some outliers on the upper end and still a very narrow distribution around 0. 


Now I will compare these models and my original model through cross-validation

```{r}
cv_df = crossv_mc(bwt_df, n = 100)
```


```{r}
cv_df = 
  cv_df %>% 
  mutate(
    my_model = map(train, \(df) lm(bwt ~ gaweeks + delwt + momage, data = df)),
    length_ga_mod = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    interactions_model = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df))
  ) %>%
  mutate(
    rmse_my_model = map2_dbl(my_model, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_length_ga = map2_dbl(length_ga_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interactions = map2_dbl(interactions_model, test, \(mod, df) rmse(model = mod, data = df))
  )
```

RMSE

```{r}
cv_summary = cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse"
  ) %>%
  group_by(model) %>%
  summarize(
    mean_rmse = mean(rmse),
    sd_rmse = sd(rmse)
  )

cv_summary %>%
  knitr::kable(digits = 3)
```

RMSE plot

```{r}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse"
  ) %>%
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

The `interactions_model` shows both a lower mean RMSE and smaller SD, both indicators of a better prediction model. 